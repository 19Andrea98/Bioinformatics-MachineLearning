{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HAG6x7zDbn_l"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Union\n",
    "from typing import Iterator, Dict\n",
    "from Bio import SeqIO\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import tarfile\n",
    "import csv\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - decompress files\n",
    ".fasta files can be packaged into a single .tar.gz archive. This part of the code is intended to extract the archive and retrieve the original .fasta files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📁 Path of the .tar.gz archive\n",
    "archive_path = r\"/home/squarna/Desktop/input/STV.tar.gz\"  \n",
    "\n",
    "# 📂 Destination folder where to extract the files\n",
    "extraction_path = r\"/home/squarna/Desktop/input\"\n",
    "\n",
    "# 🛠️ Create the folder if it does not exist\n",
    "os.makedirs(extraction_path, exist_ok=True)\n",
    "\n",
    "# 🔓 Extract the .fasta files (or all)\n",
    "with tarfile.open(archive_path, \"r:gz\") as tar:\n",
    "    for member in tar.getmembers():\n",
    "        if member.name.endswith(\".fasta\"):  # or remove this if to extract everything\n",
    "            tar.extract(member, path=extraction_path)\n",
    "    print(f\"✅ Extraction completed in: {extraction_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - merge .fasta files and create a unique file\n",
    "After decompression, you may obtain multiple .fasta files. This code merges them into a single file to facilitate downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📁 Path of the folder containing the .fasta files\n",
    "input_folder = Path(r\"/home/squarna/Desktop/input/TemPhD\")\n",
    "\n",
    "# 📤 Path of the unified file to create\n",
    "output_file = Path(r\"/home/squarna/Desktop/input/TEMPHD.fasta\")\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "    merged_count = 0\n",
    "\n",
    "    for fasta_file in input_folder.glob(\"*.fasta\"):\n",
    "        try:\n",
    "            with open(fasta_file, 'r', encoding='utf-8') as infile:\n",
    "                outfile.writelines(infile.readlines())\n",
    "                merged_count += 1\n",
    "                print(f\"✅ Aggiunto: {fasta_file.name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Errore con {fasta_file.name}: {e}\")\n",
    "\n",
    "print(f\"\\n📦 Combinati {merged_count} file in: {output_file.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - create a class to merge data and metadata\n",
    "Here, I created a class that merges each .fasta file with its corresponding metadata and removes duplicate Phage_ID entries.\n",
    "If you want to use this class for different datasets, you might need to change column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Merger:\n",
    "    def __init__(self, input_data: str, input_meta: str):\n",
    "        self.input_file = Path(input_data)\n",
    "        self.input_metafile = Path(input_meta)\n",
    "\n",
    "    def create_database(self) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Extracts IDs and sequences from a FASTA file and checks for duplicates in metadata.\"\"\"\n",
    "        protein_ids = []\n",
    "        sequences = []\n",
    "\n",
    "        with open(self.input_file) as fasta_file:\n",
    "            for seq_record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "                protein_ids.append(seq_record.id)\n",
    "                sequences.append(str(seq_record.seq))\n",
    "\n",
    "        database = pd.DataFrame({\n",
    "            \"ID\": protein_ids,\n",
    "            \"Sequence\": sequences\n",
    "        })\n",
    "\n",
    "        # remove duplicates from the ID column\n",
    "        database = database.drop_duplicates(subset=\"ID\")\n",
    "        \n",
    "        # Load metadata\n",
    "        meta_file = pd.read_csv(self.input_metafile, sep='\\t')\n",
    "\n",
    "        # Check for duplicates in Phage_ID\n",
    "        duplicates = meta_file[\"Phage_ID\"].duplicated().any()\n",
    "\n",
    "        if duplicates:\n",
    "            print(\"⚠️ There are duplicate Phage_ID entries in metadata!\")\n",
    "            meta_file = meta_file.drop_duplicates(subset=\"Phage_ID\").reset_index(drop=True)\n",
    "            print(\"✅ Duplicate Phage_ID entries removed from metadata!\")\n",
    "        else:\n",
    "            print(\"✅ No duplicates in Phage_ID within metadata.\")\n",
    "\n",
    "        return database, meta_file\n",
    "\n",
    "    def create_final_database(self, database: pd.DataFrame, meta_file: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Merges sequences and metadata into a clean final DataFrame.\"\"\"\n",
    "\n",
    "        meta_subset = meta_file[\n",
    "            ['Phage_ID', 'Length', 'GC_content', 'Taxonomy', 'Completeness', 'Host',\n",
    "             'Lifestyle', 'Cluster', 'Subcluster']\n",
    "        ]\n",
    "\n",
    "        data_completed = pd.merge(\n",
    "            database,\n",
    "            meta_subset,\n",
    "            left_on=\"ID\",\n",
    "            right_on=\"Phage_ID\",\n",
    "            how=\"inner\"\n",
    "        )\n",
    "\n",
    "        # Remove the duplicate 'Phage_ID' column\n",
    "        data_completed.drop(columns='Phage_ID', inplace=True)\n",
    "\n",
    "        # Rename the 'ID' column to 'Phage_ID'\n",
    "        data_completed.rename(columns={'ID': 'Phage_ID'}, inplace=True)\n",
    "\n",
    "        return data_completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - use the above class to create a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths for the .fasta and .tsv files\n",
    "file_path = r\"/home/squarna/Desktop/input/STV.fasta\"\n",
    "meta_path = r\"/home/squarna/Desktop/input/stv_phage_meta_data.tsv\"\n",
    "\n",
    "# Initialize the object\n",
    "merger = Merger(file_path, meta_path)\n",
    "\n",
    "# Create the DataFrames\n",
    "database, meta_file = merger.create_database()\n",
    "\n",
    "# This is the final dataset of interest\n",
    "final_data = merger.create_final_database(database, meta_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 – Check the output dataset (final_data)\n",
    "\n",
    "Metadata files (.tsv) and sequence files (.fasta) often do not have the same number of entries.  \n",
    "For this reason, it is important to check their lengths:\n",
    "\n",
    "1. **If len(file.fasta) > len(file.tsv)** → the .fasta file contains duplicated Phage_ID/sequences, or the .tsv file is missing some entries.  \n",
    "2. **If len(file.fasta) < len(file.tsv)** → the .tsv file contains duplicated Phage_ID/entries, or it has more entries than the .fasta file.  \n",
    "3. **If len(file.fasta) = len(file.tsv)** → this is a good sign; the files are most likely consistent.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(database))\n",
    "print(len(meta_file))\n",
    "print(len(final_data))\n",
    "final_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6 - save the dataset in .csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define where to save the file\n",
    "csv_output_path = r\"/home/squarna/Desktop/input/STV.csv\"\n",
    "\n",
    "# Save the file and print result\n",
    "final_data.to_csv(csv_output_path, index=False)\n",
    "\n",
    "print(f\"✅ File successfully saved in: {csv_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7 - dataset analysis\n",
    "We remove the missing values from all columns and create plots for a descriptive analysis of the dataset. In particular, we generate histograms to visualize the distribution of classes in the columns of interest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "file_path = r\"/media/ssd/Cleaned_datasets/000_dataset/000_cleaned_MIXED_dataset.csv\"\n",
    "file = pd.read_csv(file_path, sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dataset after merging operation\n",
    "file.shape, file.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values in columns\n",
    "file.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove missing values and define a new dataset\n",
    "dataset = file.dropna(how = 'any')\n",
    "dataset = dataset[dataset['Taxonomy'] != '-'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the code generates plots for the columns of interest. Column names may vary from one dataset to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define coulumn names of interest\n",
    "name = ['Taxonomy', 'Completeness', 'Host', 'Lifestyle']\n",
    "labels = {}\n",
    "\n",
    "# Define a dictionary with unique instances in each columns\n",
    "for i in name:\n",
    "    labels[i] = dataset[i].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "# Choose how many categories to keep\n",
    "top_n = 3\n",
    "\n",
    "# Calculate frequencies\n",
    "counts = dataset[\"Host\"].value_counts()\n",
    "\n",
    "# Select the top N\n",
    "top_categories = counts[:top_n]\n",
    "\n",
    "# Calculate the sum of the remaining ones\n",
    "other_count = counts[top_n:].sum()\n",
    "\n",
    "# Create a new Series with the top categories and \"Other\"\n",
    "host_summary = top_categories.copy()\n",
    "host_summary[\"Other\"] = other_count\n",
    "\n",
    "# Pie chart\n",
    "host_summary.plot(kind='pie', autopct='%1.1f%%', figsize=(6, 6))\n",
    "plt.title(\"Distribution of Host\")\n",
    "plt.ylabel('')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in [k for k in labels.keys() if k not in ['Host', 'Taxonomy']]:\n",
    "    counts = dataset[column].value_counts()\n",
    "    total = counts.sum()\n",
    "    \n",
    "    # Create labels with percentages\n",
    "    labels_with_pct = [\n",
    "        f\"{name} ({count / total:.1%})\" for name, count in zip(counts.index, counts)\n",
    "    ]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    wedges, texts = ax.pie(counts, startangle=90)  # no autopct, percentages will be shown in the legend\n",
    "    ax.legend(wedges, labels_with_pct, title=column, loc=\"center left\", bbox_to_anchor=(1, 0, 0.5, 1))\n",
    "\n",
    "    plt.title(f'Distribution of {column}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in [k for k in labels.keys() if k not in ['Host', 'Lifestyle', 'Completeness']]:\n",
    "    counts = dataset[column].value_counts()\n",
    "    total = counts.sum()\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    bars = plt.bar(counts.index.astype(str), counts.values)\n",
    "\n",
    "    # Add percentages above the bars\n",
    "    for bar, count in zip(bars, counts):\n",
    "        percent = count / total * 100\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(),\n",
    "                 f'{percent:.1f}%', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    plt.title(f'Distribution of {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If bar plot is too messy do to numerous classes try this part of the code to make a better graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in [k for k in labels.keys() if k not in ['Host', 'Lifestyle', 'Completeness']]:\n",
    "    counts = dataset[column].value_counts()\n",
    "\n",
    "    # Select the top 8 classes\n",
    "    top_classes = counts.head(8)\n",
    "    others_sum = counts[8:].sum()\n",
    "\n",
    "    # Add the \"Other\" class if necessary\n",
    "    if others_sum > 0:\n",
    "        counts_reduced = top_classes.copy()\n",
    "        counts_reduced['Other'] = others_sum\n",
    "    else:\n",
    "        counts_reduced = top_classes\n",
    "\n",
    "    total = counts_reduced.sum()\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    bars = plt.bar(counts_reduced.index.astype(str), counts_reduced.values)\n",
    "\n",
    "    # Add percentages above the bars\n",
    "    for bar, count in zip(bars, counts_reduced):\n",
    "        percent = count / total * 100\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(),\n",
    "                 f'{percent:.1f}%', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    plt.title(f'Distribution of {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8 - check distribution\n",
    "Do it if necessary --> you can check the distribution of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "import seaborn as sns\n",
    "\n",
    "# Categories of interest\n",
    "categories = ['caudovirales', 'inoviridae', 'microviridae', 'riboviria']\n",
    "\n",
    "# --- First plot: global distribution ---\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(dataset['GC_content'].dropna(), bins=30, kde=True, color='steelblue')\n",
    "plt.title('Global distribution of GC_content')\n",
    "plt.xlabel('GC_content (%)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Separate plots for each category ---\n",
    "for cat in categories:\n",
    "    subset = dataset[dataset['Taxonomy'].str.lower().str.contains(cat, na=False)]\n",
    "    gc_values = subset['GC_content'].dropna()\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.histplot(gc_values, bins=30, kde=True)\n",
    "    plt.title(f'Distribution of GC_content for {cat.capitalize()}')\n",
    "    plt.xlabel('GC_content (%)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9 - final dataset \n",
    "Use this part of code to remove Lo-quality and Not-determined sequences (Completeness).\n",
    "Moreover, remove illegal protein sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the dataset removing Low-qualited and Not-determined sequences\n",
    "final_dataset = dataset.drop(\n",
    "    dataset[dataset['Completeness'].isin(['Low-quality', 'Not-determined'])].index\n",
    ").set_index('Phage_ID')\n",
    "\n",
    "final_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9.1 - illegal sequences\n",
    "Use this part of code to remove illegal sequences (sequences with illegal character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to identify valid sequences\n",
    "def is_valid_sequence(seq, allowed=\"ACDEFGHIKLMNPQRSTVWY\"):\n",
    "    return re.fullmatch(f\"[{allowed}]+\", seq) is not None\n",
    "\n",
    "def clean_invalid_sequences(input_path, output_path, invalid_output_path):\n",
    "    df = pd.read_csv(input_path)\n",
    "\n",
    "    # Validity mask\n",
    "    valid_mask = df[\"Sequence\"].apply(is_valid_sequence)\n",
    "\n",
    "    # Separation\n",
    "    valid_df = df[valid_mask].reset_index(drop=True)\n",
    "    invalid_df = df[~valid_mask].reset_index(drop=True)\n",
    "\n",
    "    # Saving\n",
    "    valid_df.to_csv(output_path, index=False)\n",
    "    invalid_df.to_csv(invalid_output_path, index=False)\n",
    "\n",
    "    print(f\"✅ Valid sequences: {len(valid_df)} saved in {output_path}\")\n",
    "    print(f\"❌ Invalid sequences: {len(invalid_df)} saved in {invalid_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_invalid_sequences(\n",
    "    input_path = file_path,\n",
    "    output_path = r'/home/squarna/Desktop/csssleaned_MIXED_dataset.csv',\n",
    "    invalid_output_path = r'/home/squarna/Desktop/cmerdaaaaaaleaned_MIXED_dataset.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned dataset (no illegal sequences and low-quality/not-determined sequences)\n",
    "final_dataset.to_csv('/home/squarna/Desktop/cleaned_MIXED_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final check 🤪\n",
    "counter = 0\n",
    "\n",
    "for i in range(len(final_dataset)):\n",
    "    if final_dataset['Completeness'].iloc[i] in (['Low-quality', 'Not-determined']):\n",
    "        counter += 1\n",
    "\n",
    "if counter == 0:\n",
    "    print(\"✅ Ok\")\n",
    "else:\n",
    "    print(f\"⚠️ KO {counter}\")\n",
    "\n",
    "\n",
    "                                       "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
